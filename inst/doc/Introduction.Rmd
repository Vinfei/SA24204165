---
title: "Introduction"
author: "吴鹏飞"
date: "2023-12-09"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## 一.理论背景介绍：高维回归、LASSO 和置信区间

### 1. **高维回归问题**
高维回归指的是数据维度 \( p \) 大于甚至远大于样本数 \( n \) 的回归问题。在这种场景下，传统的线性回归会遇到以下困难：
- **多重共线性**：协变量之间可能高度相关，导致回归系数估计不稳定。
- **过拟合**：自由度过高，模型会过度拟合噪声。
- **无法唯一解**：当 \( p > n \) 时，设计矩阵 \( X \) 的列不可逆，回归方程没有唯一解。

### 2. **LASSO 回归**
LASSO（Least Absolute Shrinkage and Selection Operator）是一种常用的高维回归方法，目标是通过正则化约束来克服上述问题。其目标函数为：
\[
\hat{\beta} = \arg\min_{\beta} \left( \frac{1}{2n} \|y - X\beta\|_2^2 + \lambda \|\beta\|_1 \right),
\]
其中：
- \( \|y - X\beta\|_2^2 \)：残差平方和，衡量拟合优度。
- \( \|\beta\|_1 = \sum_{j=1}^p |\beta_j| \)：LASSO 正则项，通过惩罚系数 \( \lambda \) 控制稀疏性。
  
LASSO 的优点：
- 自动进行变量选择：能将不重要的回归系数收缩为零。
- 适用于高维数据：能稳定处理 \( p \gg n \) 的场景。
  
LASSO 的一个缺点是它引入了估计偏差（bias），需要后续调整以获得无偏估计。

### 3. **置信区间与无偏估计**
在高维回归中，为了进行更可靠的统计推断，需要对回归系数估计量构造置信区间。挑战包括：
- **噪声估计**：需要准确估计模型中的噪声方差。
- **协方差矩阵的求解**：需要高效地逆解协方差矩阵或其稀疏表示。

本代码通过结合稀疏矩阵逆解法（`compute_inverse`）和无偏校正方法，计算回归系数的无偏估计以及对应的置信区间，提供对高维数据的稳健推断。

### 4. **代码中实现的主要功能**
代码中的功能模块体现了上述理论的实际实现，包括：

(1) **SSLASSO**：main函数，进行 LASSO 回归、无偏估计修正，并生成置信区间。

(2) **稀疏矩阵求逆**：通过约束优化（`solve_one_row` 和 `compute_inverse`），高效计算协方差矩阵的稀疏逆矩阵。

(3) **噪声估计**：通过残差和协方差矩阵估计噪声的标准差，辅助置信区间的计算。

(4) **LASSO 求解**：基于 `glmnet` 的 LASSO 实现，支持灵活的正则化参数选择。

(5) **soft_threshold**：实现软阈值运算，对输入值执行稀疏性约束。将大于正阈值的值减少阈值，小于负阈值的值增加阈值，绝对值小于阈值的值置为 0。用于 LASSO 和其他稀疏优化问题，帮助实现系数的稀疏化。

(6) **solve_one_row**：在稀疏矩阵的逆解过程中，逐行求解约束优化问题。通过迭代法解决协方差矩阵中每一行的优化问题，保证解的稀疏性。将矩阵逆解问题分解为更小的计算单元，提高高维矩阵稀疏逆解的效率和准确性。

```{r}
library(knitr)
library(ggplot2)
library(boot)
library(bootstrap)
library(DAAG)
library(MASS)
library(coda)
library(Rcpp)
library(microbenchmark)
library(truncnorm)
library(stats)
library(glmnet)
library(expm)
library(flare)
```

```{r}
#' @title High-dimensional Regression with LASSO and Confidence Intervals
#' @description Implements a high-dimensional regression procedure using LASSO, computes confidence intervals, and provides unbiased estimates.
#' @param X A numeric matrix of predictors.
#' @param y A numeric vector of response values.
#' @param alpha Significance level for confidence intervals (default: 0.05).
#' @param lambda Regularization parameter for LASSO (default: NULL, calculated internally).
#' @param max_iter Maximum number of iterations for optimization algorithms (default: 50).
#' @param tol Convergence tolerance for optimization (default: 1e-2).
#' @param verbose Logical; if TRUE, displays progress (default: TRUE).
#' @return A list containing:
#'   \item{estimates}{LASSO regression coefficients.}
#'   \item{unbiased_estimates}{Unbiased regression coefficients.}
#'   \item{confidence_intervals}{Matrix of confidence intervals for the coefficients.}
#'   \item{noise_sd}{Estimated standard deviation of noise.}
#' @examples
#' {
#' set.seed(123)
#' n <- 100
#' p <- 50
#' X <- matrix(rnorm(n * p), n, p)
#' beta_true <- c(rep(2, 5), rep(0, p - 5))
#' y <- X %*% beta_true + rnorm(n)
#' result <- SSLASSO(X, y, alpha = 0.05)
#' print(result$estimates)
#' print(result$unbiased_estimates)
#' print(result$confidence_intervals)
#' }
#' @name SSLASSO
#' @export
SSLASSO <- function(X, y, alpha = 0.05, lambda = NULL, max_iter = 50, tol = 1e-2, verbose = TRUE) {
  n <- nrow(X)
  p <- ncol(X)
  scaled_X <- scale(X, scale = FALSE)
  design_norm <- sqrt(colSums(scaled_X^2) / n)
  scaled_X <- sweep(scaled_X, 2, design_norm, `/`)
  
  # Get Lasso regression coefficients
  lasso_fit <- Lasso(scaled_X, y, lambda = lambda)
  
  # Ensure lasso_fit is a vector with length equal to p
  if (length(lasso_fit) != p) {
    stop("Lasso() output should be a vector of length p.")
  }
  
  cov_matrix <- crossprod(scaled_X) / n
  inverse_matrix <- compute_inverse(cov_matrix, n, max_iter = max_iter, tol = tol, verbose = verbose)
  unbiased_estimates <- lasso_fit + inverse_matrix %*% crossprod(scaled_X, y - scaled_X %*% lasso_fit) / n
  
  weight_matrix <- inverse_matrix %*% cov_matrix %*% inverse_matrix
  noise_est <- Noise(unbiased_estimates, weight_matrix, n)
  
  interval_half_width <- qnorm(1 - alpha / 2) * noise_est$noise_sd * sqrt(diag(weight_matrix)) / sqrt(n)
  
  list(
    estimates = lasso_fit,
    unbiased_estimates = unbiased_estimates,
    confidence_intervals = cbind(
      unbiased_estimates - interval_half_width,
      unbiased_estimates + interval_half_width
    ),
    noise_sd = noise_est$noise_sd
  )
}

#' @title Soft Threshold Function
#' @description Applies a soft-thresholding operation to a given value.
#' @param value Numeric; the input value to threshold.
#' @param threshold Numeric; the threshold value.
#' @return A numeric value after applying the soft-threshold.
#' @examples
#' {
#' result <- soft_threshold(3.5, 2)
#' print(result)
#' }
#' @name soft_threshold
#' @export
soft_threshold <- function(value, threshold) {
  if (value > threshold) {
    return(value - threshold)
  } else if (value < -threshold) {
    return(value + threshold)
  } else {
    return(0)
  }
}

#' @title Solve One Row for Constrained Optimization
#' @description Solves a single row of a covariance matrix for constrained optimization in high-dimensional regression.
#' @param cov_matrix Numeric matrix; the covariance matrix.
#' @param row_idx Integer; index of the row to solve.
#' @param constraint Numeric; the constraint parameter.
#' @param max_iter Integer; maximum number of iterations (default: 50).
#' @param tol Numeric; convergence tolerance (default: 1e-2).
#' @return A list containing:
#'   \item{solution}{The solution vector for the row.}
#'   \item{iterations}{Number of iterations performed.}
#' @examples
#' {
#' cov_matrix <- matrix(c(4, 2, 2, 3), nrow = 2)
#' result <- solve_one_row(cov_matrix, 1, 0.1)
#' print(result)
#' }
#' @name solve_one_row
#' @export
solve_one_row <- function(cov_matrix, row_idx, constraint, max_iter = 50, tol = 1e-2) {
  n <- nrow(cov_matrix)
  rho <- max(abs(cov_matrix[row_idx, -row_idx])) / cov_matrix[row_idx, row_idx]
  initial_constraint <- rho / (1 + rho)
  coeff <- numeric(n)
  
  if (constraint >= initial_constraint) {
    coeff[row_idx] <- (1 - initial_constraint) / cov_matrix[row_idx, row_idx]
    return(list(solution = coeff, iterations = 0))
  }
  
  residual <- -cov_matrix %*% coeff
  residual[row_idx] <- residual[row_idx] + 1
  iter <- 1
  prev_coeff <- coeff
  
  while (iter <= max_iter) {
    for (j in 1:n) {
      residual[j] <- residual[j] + coeff[j] * cov_matrix[j, j]
      coeff[j] <- soft_threshold(residual[j], constraint) / cov_matrix[j, j]
      residual[j] <- residual[j] - coeff[j] * cov_matrix[j, j]
    }
    
    if (sqrt(sum((coeff - prev_coeff)^2)) < tol * sqrt(sum(coeff^2))) {
      break
    }
    
    prev_coeff <- coeff
    iter <- iter + 1
  }
  
  list(solution = coeff, iterations = iter)
}

#' @title Compute Sparse Inverse Matrix
#' @description Computes the sparse inverse of a covariance matrix using row-wise constrained optimization.
#' @param cov_matrix Numeric matrix; the covariance matrix.
#' @param sample_size Integer; the number of samples.
#' @param constraint Numeric; constraint parameter (default: NULL, calculated internally).
#' @param max_iter Integer; maximum number of iterations (default: 50).
#' @param tol Numeric; convergence tolerance (default: 1e-2).
#' @param verbose Logical; if TRUE, displays progress (default: TRUE).
#' @return A numeric matrix representing the sparse inverse of the input matrix.
#' @examples
#' {
#' cov_matrix <- matrix(c(4, 2, 2, 3), nrow = 2)
#' result <- compute_inverse(cov_matrix, sample_size = 100)
#' print(result)
#' }
#' @name compute_inverse
#' @export
compute_inverse <- function(cov_matrix, sample_size, constraint = NULL, max_iter = 50, tol = 1e-2, verbose = TRUE) {
  p <- nrow(cov_matrix)
  inverse_matrix <- matrix(0, nrow = p, ncol = p)
  
  if (is.null(constraint)) {
    constraint <- qnorm(1 - 0.1 / p^2) / sqrt(sample_size)
  }
  
  for (i in 1:p) {
    if (verbose && i %% (p / 10) == 0) {
      cat(sprintf("%d%% completed\n", round(i / p * 100)))
    }
    
    row_solution <- solve_one_row(cov_matrix, i, constraint, max_iter, tol)
    inverse_matrix[i, ] <- row_solution$solution
  }
  
  inverse_matrix
}

#' @title Fit LASSO Model
#' @description Fits a LASSO regression model using either glmnet or square-root Lasso.
#' @param X A numeric matrix of predictors.
#' @param y A numeric vector of response values.
#' @param lambda Regularization parameter for LASSO (default: NULL).
#' @param intercept Logical; whether to include an intercept in the model (default: TRUE).
#' @return A numeric vector of LASSO regression coefficients.
#' @examples
#' {
#' X <- matrix(rnorm(100 * 10), nrow = 100)
#' y <- rnorm(100)
#' result <- Lasso(X, y)
#' print(result)
#' }
#' @name Lasso
#' @export
Lasso <- function(X, y, lambda = NULL, intercept = TRUE) {
  p <- ncol(X)
  n <- nrow(X)

  if (is.null(lambda)) {
    lambda <- sqrt(qnorm(1 - (0.1 / p)) / n)
  }

  outLas <- glmnet(X, y, family = "gaussian", alpha = 1, intercept = intercept, lambda = lambda)
  
  if (intercept) {
    coef_out <- as.vector(coef(outLas, s = lambda))
    return(coef_out[-1])  
  } else {
    return(as.vector(coef(outLas, s = lambda))[-1])  
  }
}

#' @title Estimate Noise Standard Deviation
#' @description Estimates the noise standard deviation using the residuals.
#' @param yh Numeric vector; the predicted values or regression coefficients.
#' @param A Numeric matrix; the covariance matrix (or similar matrix).
#' @param n Integer; the number of samples.
#' @return A list containing:
#'   \item{noise_sd}{Estimated noise standard deviation.}
#'   \item{nz}{Number of non-zero coefficients or significant coefficients.}
#' @name Noise
#' @export
Noise <- function(yh, A, n) {
  ynorm <- sqrt(n) * (yh / sqrt(diag(A)))
  sd_hat0 <- mad(ynorm)
  
  zeros <- (abs(ynorm) < 3 * sd_hat0)
  y2norm <- sum(yh[zeros]^2)
  Atrace <- sum(diag(A)[zeros])
  sd_hat1 <- sqrt(n * y2norm / Atrace)
  
  ratio <- sd_hat0 / sd_hat1
  if (max(ratio, 1 / ratio) > 2) {
    print("Warning: Noise estimate problematic")
  }
  
  s0 <- sum(zeros == FALSE)
  return(list("noise_sd" = sd_hat1, "nz" = s0))
}

```

### 5. **实例**
下面是一个高维回归模型的示例，使用了LASSO（最小绝对收缩和选择算子）回归并计算了回归系数的置信区间
```{r}
set.seed(123)
n <- 100
p <- 50
X <- matrix(rnorm(n * p), n, p)
beta_true <- c(rep(2, 5), rep(0, p - 5))
y <- X %*% beta_true + rnorm(n)
result <- SSLASSO(X, y, alpha = 0.05)
print(result$estimates)
print(result$unbiased_estimates)
print(result$confidence_intervals)
```

## 二.理论背景介绍：隐马尔科夫模型（HMM）参数估计

隐马尔科夫模型（Hidden Markov Model, HMM）是一种用于建模时间序列数据的概率模型，广泛应用于语音识别、基因组学、行为建模等领域。HMM 的核心思想是：观测数据由一个隐藏状态的马尔科夫链生成，每个隐藏状态通过特定的概率分布生成观测数据。这种模型可以捕捉数据的序列依赖性和复杂的生成过程。

### 1.HMM 的基本组成

(1) **隐藏状态（Hidden States）：**  
   系统在任意时间点处于一个隐藏状态，这些状态不可直接观测，只能通过观测数据推测。

(2) **观测序列（Observations）：**  
   每个时间点的观测值由隐藏状态生成。观测值的分布形式由模型假定（如正态分布或泊松分布）。

(3) **模型参数：**  
   - 状态转移概率矩阵 \( A \)：描述隐藏状态间的转移概率。
   - 初始状态分布 \( \pi \)：描述系统开始时隐藏状态的分布。
   - 观测分布参数（如均值 \( \mu \) 和标准差 \( \sigma \)）：定义隐藏状态生成观测值的分布形式。

### 2.HMM 的参数估计

为了应用 HMM，需要估计上述参数。参数估计通常通过 **期望最大化（EM）算法** 实现：

(1) **E 步（期望步）：**  
   给定当前参数估计，计算隐藏状态的后验概率（即数据生成每个隐藏状态的可能性）。

(2) **M 步（最大化步）：**  
   根据隐藏状态的后验概率更新参数，以最大化模型的对数似然函数。

EM 算法反复迭代，直到收敛到最优解。

### 3.本代码的实现功能

代码基于 `Rcpp` 实现了使用 EM 算法对 HMM 参数的估计，支持泊松分布和正态分布两种观测分布。其功能包括：

(1) 初始化 HMM 参数（状态转移矩阵、初始状态分布、观测分布的均值和标准差）。

(2) 在 EM 算法中：
   - **E 步：** 计算后验概率矩阵 \( \gamma \)，表示每个时间点属于某个隐藏状态的概率。
   - **M 步：** 更新状态转移矩阵 \( A \)、初始状态分布 \( \pi \)、观测分布的均值 \( \mu \)，以及标准差 \( \sigma \)。

(3) 输出优化后的参数结果，包括 \( A \)、\( \pi \)、\( \mu \) 和 \( \sigma \)（仅对正态分布）。

```{Rcpp}
#include <Rcpp.h>
#include <cmath>
#include <vector>
using namespace Rcpp;

// [[Rcpp::export]]
List hmm_em(NumericMatrix observations, int n_states, std::string dist_type = "normal", int max_iter = 100) {
   int n_obs = observations.nrow();
   int n_features = observations.ncol();
   
   // Initialize parameters
   NumericMatrix A(n_states, n_states); // Transition matrix
   NumericVector pi(n_states, 1.0 / n_states); // Initial state probabilities
   NumericMatrix mu(n_states, n_features); // Mean matrix
   NumericMatrix sigma(n_states, n_features); // Standard deviation matrix (for normal distribution)
   
   // Uniform initialization for A
   for (int i = 0; i < n_states; ++i) {
     for (int j = 0; j < n_states; ++j) {
       A(i, j) = 1.0 / n_states;
     }
   }
   
   // Random initialization for means and standard deviations
   for (int k = 0; k < n_states; ++k) {
     for (int d = 0; d < n_features; ++d) {
       mu(k, d) = R::runif(0, 10);
       sigma(k, d) = R::runif(1, 5);
     }
   }
   
   // Variables for the E-step
   NumericMatrix gamma(n_obs, n_states); // Posterior probability matrix
   
   // EM iteration
   for (int iter = 0; iter < max_iter; ++iter) {
     // --- E-step ---
     for (int t = 0; t < n_obs; ++t) {
       double row_sum = 0.0;
       for (int k = 0; k < n_states; ++k) {
         double likelihood = 1.0;
         for (int d = 0; d < n_features; ++d) {
           if (dist_type == "poisson") {
             likelihood *= R::dpois(observations(t, d), mu(k, d), false);
           } else if (dist_type == "normal") {
             likelihood *= R::dnorm(observations(t, d), mu(k, d), sigma(k, d), false);
           }
         }
         // Prevent multiplication overflow or underflow
         if (likelihood > 0) {
           gamma(t, k) = pi[k] * likelihood;
         } else {
           gamma(t, k) = 0;
         }
         row_sum += gamma(t, k);
       }
       // Normalize
       if (row_sum > 0) {
         for (int k = 0; k < n_states; ++k) {
           gamma(t, k) /= row_sum;
         }
       } else {
         for (int k = 0; k < n_states; ++k) {
           gamma(t, k) = 1.0 / n_states;  // Avoid division by zero
         }
       }
     }
     
     // --- M-step ---
     // Update initial state probabilities
     for (int k = 0; k < n_states; ++k) {
       pi[k] = gamma(0, k);
     }
     
     // Update transition matrix
     for (int i = 0; i < n_states; ++i) {
       for (int j = 0; j < n_states; ++j) {
         double num = 0.0, denom = 0.0;
         for (int t = 0; t < n_obs - 1; ++t) {
           num += gamma(t, i) * gamma(t + 1, j);
           denom += gamma(t, i);
         }
         if (denom > 0) {
           A(i, j) = num / denom;
         } else {
           A(i, j) = 1.0 / n_states;  // Avoid division by zero
         }
       }
     }
     
     // Update means and standard deviations
     for (int k = 0; k < n_states; ++k) {
       for (int d = 0; d < n_features; ++d) {
         double num_mu = 0.0, denom_mu = 0.0, num_sigma = 0.0;
         for (int t = 0; t < n_obs; ++t) {
           num_mu += gamma(t, k) * observations(t, d);
           denom_mu += gamma(t, k);
         }
         if (denom_mu > 0) {
           mu(k, d) = num_mu / denom_mu;
         } else {
           mu(k, d) = R::runif(0, 10); // Default if no observation
         }
         
         if (dist_type == "normal") {
           for (int t = 0; t < n_obs; ++t) {
             num_sigma += gamma(t, k) * std::pow(observations(t, d) - mu(k, d), 2);
           }
           if (denom_mu > 0) {
             sigma(k, d) = std::sqrt(num_sigma / denom_mu);
           } else {
             sigma(k, d) = R::runif(1, 5); // Default if no observation
           }
         }
       }
     }
   }
   
   return List::create(
     Named("A") = A,
     Named("pi") = pi,
     Named("mu") = mu,
     Named("sigma") = sigma
   );
}

```


```{r}
# 创建模拟数据
set.seed(123)
n_obs <- 100
n_features <- 2
observations <- matrix(rnorm(n_obs * n_features), ncol = n_features)

# 使用 HMM EM 算法进行训练
n_states <- 3
result <- hmm_em(observations, n_states, dist_type = "normal", max_iter = 50)

# 查看结果
result$A  # 转移矩阵
result$pi  # 初始状态概率
result$mu  # 均值矩阵
result$sigma  # 标准差矩阵

```
### 4.应用场景

适用于以下情况：
- 时间序列数据中存在潜在的隐藏状态。
- 观测分布可通过泊松分布或正态分布近似。
- 需要通过数据驱动方式估计 HMM 的模型参数。